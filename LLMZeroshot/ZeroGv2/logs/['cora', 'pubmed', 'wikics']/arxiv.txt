2025-01-08 03:36:34,211 - lyh - INFO - welcome!!!!
2025-01-08 03:36:34,841 - lyh - INFO - [STAGE 1] Loading All Test Graphs ['cora', 'pubmed', 'wikics'] ...
2025-01-08 03:36:35,474 - lyh - INFO - [STAGE 2] Preparing Text Encoder SentenceBert with # Trainable Parameters 73728 ...
2025-01-08 03:40:07,512 - lyh - INFO - welcome!!!!
2025-01-08 03:40:08,345 - lyh - INFO - [STAGE 1] Loading All Test Graphs ['cora', 'pubmed', 'wikics'] ...
2025-01-08 03:40:08,932 - lyh - INFO - [STAGE 2] Preparing Text Encoder SentenceBert with # Trainable Parameters 73728 ...
2025-01-08 03:40:12,679 - lyh - INFO - [STAGE 3] Loading Training Subgraphs from ['arxiv'], forming 144 train-loaders ...
2025-01-08 03:42:33,151 - lyh - INFO - [([60.89, 55.54], {0: [55.17, 51.08], 1: [58.67, 54.63], 2: [60.33, 56.17], 3: [61.44, 57.04], 4: [61.62, 57.25], 5: [61.07, 56.41], 6: [61.44, 56.81], 7: [62.18, 57.36], 8: [61.81, 56.64], 9: [60.89, 55.54]}), ([74.29, 72.8], {0: [58.39, 58.17], 1: [64.25, 63.87], 2: [66.71, 66.17], 3: [68.91, 68.25], 4: [71.37, 70.56], 5: [73.15, 72.28], 6: [73.78, 72.79], 7: [74.14, 73.05], 8: [74.09, 72.81], 9: [74.29, 72.8]}), ([52.84, 47.83], {0: [57.33, 53.42], 1: [61.21, 56.44], 2: [62.62, 57.2], 3: [62.84, 56.88], 4: [62.49, 55.95], 5: [62.28, 55.31], 6: [61.13, 53.89], 7: [58.61, 52.11], 8: [56.86, 51.06], 9: [52.84, 47.83]})]
2025-01-08 03:42:33,152 - lyh - INFO - batch: 0
2025-01-08 03:42:34,333 - lyh - INFO - step: 3, loss: 108.8968734741211
2025-01-08 03:42:35,437 - lyh - INFO - step: 7, loss: 119.17617797851562
2025-01-08 03:42:36,736 - lyh - INFO - step: 11, loss: 114.01407623291016
2025-01-08 03:42:37,865 - lyh - INFO - step: 15, loss: 92.22377014160156
2025-01-08 03:42:38,787 - lyh - INFO - step: 19, loss: 87.43412780761719
2025-01-08 03:42:40,022 - lyh - INFO - step: 23, loss: 112.38070678710938
2025-01-08 03:42:41,194 - lyh - INFO - step: 27, loss: 84.46927642822266
2025-01-08 03:42:42,163 - lyh - INFO - step: 31, loss: 119.68875122070312
2025-01-08 03:42:43,426 - lyh - INFO - step: 35, loss: 107.912353515625
2025-01-08 03:42:44,649 - lyh - INFO - step: 39, loss: 85.95369720458984
2025-01-08 03:42:45,877 - lyh - INFO - step: 43, loss: 84.1556625366211
2025-01-08 03:42:47,105 - lyh - INFO - step: 47, loss: 68.52661895751953
2025-01-08 03:42:48,123 - lyh - INFO - step: 51, loss: 104.32479095458984
2025-01-08 03:42:49,279 - lyh - INFO - step: 55, loss: 68.76408386230469
2025-01-08 03:42:50,489 - lyh - INFO - step: 59, loss: 76.32919311523438
2025-01-08 03:42:51,450 - lyh - INFO - step: 63, loss: 70.95462799072266
2025-01-08 03:42:52,242 - lyh - INFO - step: 67, loss: 71.974853515625
2025-01-08 03:42:53,415 - lyh - INFO - step: 71, loss: 53.578914642333984
2025-01-08 03:42:54,398 - lyh - INFO - step: 75, loss: 89.3403549194336
2025-01-08 03:42:55,340 - lyh - INFO - step: 79, loss: 81.87702178955078
2025-01-08 03:42:56,163 - lyh - INFO - step: 83, loss: 73.27759552001953
2025-01-08 03:42:57,106 - lyh - INFO - step: 87, loss: 75.83464813232422
2025-01-08 03:42:58,148 - lyh - INFO - step: 91, loss: 63.079830169677734
2025-01-08 03:42:58,976 - lyh - INFO - step: 95, loss: 55.082252502441406
2025-01-08 03:43:00,021 - lyh - INFO - step: 99, loss: 59.118961334228516
2025-01-08 03:43:01,119 - lyh - INFO - step: 103, loss: 65.37842559814453
2025-01-08 03:43:01,986 - lyh - INFO - step: 107, loss: 80.07842254638672
2025-01-08 03:43:03,085 - lyh - INFO - step: 111, loss: 74.9025650024414
2025-01-08 03:43:03,874 - lyh - INFO - step: 115, loss: 44.70272445678711
2025-01-08 03:43:04,687 - lyh - INFO - step: 119, loss: 84.38916778564453
2025-01-08 03:43:05,552 - lyh - INFO - step: 123, loss: 33.99009323120117
2025-01-08 03:43:06,289 - lyh - INFO - step: 127, loss: 42.61087417602539
2025-01-08 03:43:07,341 - lyh - INFO - step: 131, loss: 51.473663330078125
2025-01-08 03:43:08,128 - lyh - INFO - step: 135, loss: 49.663455963134766
2025-01-08 03:43:09,410 - lyh - INFO - step: 139, loss: 55.28950119018555
2025-01-08 03:43:10,048 - lyh - INFO - step: 143, loss: 38.542213439941406
2025-01-08 03:43:10,050 - lyh - INFO - time: 36.897960901260376
2025-01-08 03:43:18,899 - lyh - INFO - [MODEL EVAL] Epoch 001 idx is 0 {0: [54.43, 50.68], 1: [57.93, 53.94], 2: [59.78, 55.46], 3: [60.7, 56.19], 4: [61.07, 56.31], 5: [61.25, 56.54], 6: [61.81, 56.98], 7: [62.36, 57.42], 8: [62.36, 57.48], 9: [61.25, 55.99]}
2025-01-08 03:44:39,478 - lyh - INFO - [MODEL EVAL] Epoch 001 idx is 1 {0: [58.16, 57.86], 1: [64.91, 64.42], 2: [66.78, 66.13], 3: [69.57, 68.75], 4: [71.65, 70.67], 5: [73.02, 72.02], 6: [73.66, 72.56], 7: [74.04, 72.79], 8: [74.19, 72.74], 9: [74.06, 72.39]}
2025-01-08 03:47:48,814 - lyh - INFO - welcome!!!!
2025-01-08 03:47:49,660 - lyh - INFO - [STAGE 1] Loading All Test Graphs ['cora', 'pubmed', 'wikics'] ...
2025-01-08 03:47:50,249 - lyh - INFO - [STAGE 2] Preparing Text Encoder SentenceBert with # Trainable Parameters 73728 ...
2025-01-08 03:47:54,327 - lyh - INFO - [STAGE 3] Loading Training Subgraphs from ['arxiv'], forming 144 train-loaders ...
2025-01-08 03:50:14,369 - lyh - INFO - [([60.89, 55.54], {0: [55.17, 51.08], 1: [58.67, 54.63], 2: [60.33, 56.17], 3: [61.44, 57.04], 4: [61.62, 57.25], 5: [61.07, 56.41], 6: [61.44, 56.81], 7: [62.18, 57.36], 8: [61.81, 56.64], 9: [60.89, 55.54]}), ([74.29, 72.8], {0: [58.39, 58.17], 1: [64.25, 63.87], 2: [66.71, 66.17], 3: [68.91, 68.25], 4: [71.37, 70.56], 5: [73.15, 72.28], 6: [73.78, 72.79], 7: [74.14, 73.05], 8: [74.09, 72.81], 9: [74.29, 72.8]}), ([52.84, 47.83], {0: [57.33, 53.42], 1: [61.21, 56.44], 2: [62.62, 57.2], 3: [62.84, 56.88], 4: [62.49, 55.95], 5: [62.28, 55.31], 6: [61.13, 53.89], 7: [58.61, 52.11], 8: [56.86, 51.06], 9: [52.84, 47.83]})]
2025-01-08 03:50:14,370 - lyh - INFO - batch: 0
2025-01-08 03:50:15,550 - lyh - INFO - step: 3, loss: 108.8968734741211
2025-01-08 03:50:16,657 - lyh - INFO - step: 7, loss: 119.17610168457031
2025-01-08 03:50:17,968 - lyh - INFO - step: 11, loss: 114.01407623291016
2025-01-08 03:50:19,098 - lyh - INFO - step: 15, loss: 92.22377014160156
2025-01-08 03:50:20,027 - lyh - INFO - step: 19, loss: 87.43403625488281
2025-01-08 03:50:21,259 - lyh - INFO - step: 23, loss: 112.38068389892578
2025-01-08 03:50:22,429 - lyh - INFO - step: 27, loss: 84.46928405761719
2025-01-08 03:50:23,395 - lyh - INFO - step: 31, loss: 119.68892669677734
2025-01-08 03:50:24,661 - lyh - INFO - step: 35, loss: 107.91239929199219
2025-01-08 03:50:25,882 - lyh - INFO - step: 39, loss: 85.95365905761719
2025-01-08 03:50:27,123 - lyh - INFO - step: 43, loss: 84.15560913085938
2025-01-08 03:50:28,359 - lyh - INFO - step: 47, loss: 68.52661895751953
2025-01-08 03:50:29,387 - lyh - INFO - step: 51, loss: 104.32489013671875
2025-01-08 03:50:30,547 - lyh - INFO - step: 55, loss: 68.76397705078125
2025-01-08 03:50:31,757 - lyh - INFO - step: 59, loss: 76.32926177978516
2025-01-08 03:50:32,722 - lyh - INFO - step: 63, loss: 70.95464324951172
2025-01-08 03:50:33,522 - lyh - INFO - step: 67, loss: 71.97492218017578
2025-01-08 03:50:34,693 - lyh - INFO - step: 71, loss: 53.57889175415039
2025-01-08 03:50:35,675 - lyh - INFO - step: 75, loss: 89.34022521972656
2025-01-08 03:50:36,620 - lyh - INFO - step: 79, loss: 81.87696075439453
2025-01-08 03:50:37,449 - lyh - INFO - step: 83, loss: 73.27760314941406
2025-01-08 03:50:38,398 - lyh - INFO - step: 87, loss: 75.83467864990234
2025-01-08 03:50:39,441 - lyh - INFO - step: 91, loss: 63.0797004699707
2025-01-08 03:50:40,268 - lyh - INFO - step: 95, loss: 55.08210372924805
2025-01-08 03:50:41,327 - lyh - INFO - step: 99, loss: 59.11898422241211
2025-01-08 03:50:42,435 - lyh - INFO - step: 103, loss: 65.37834930419922
2025-01-08 03:50:43,304 - lyh - INFO - step: 107, loss: 80.07831573486328
2025-01-08 03:50:44,403 - lyh - INFO - step: 111, loss: 74.90250396728516
2025-01-08 03:50:45,192 - lyh - INFO - step: 115, loss: 44.70271301269531
2025-01-08 03:50:46,006 - lyh - INFO - step: 119, loss: 84.38917541503906
2025-01-08 03:50:46,872 - lyh - INFO - step: 123, loss: 33.99005126953125
2025-01-08 03:50:47,609 - lyh - INFO - step: 127, loss: 42.61095428466797
2025-01-08 03:50:48,667 - lyh - INFO - step: 131, loss: 51.4736328125
2025-01-08 03:50:49,451 - lyh - INFO - step: 135, loss: 49.663394927978516
2025-01-08 03:50:50,734 - lyh - INFO - step: 139, loss: 55.28950881958008
2025-01-08 03:50:51,375 - lyh - INFO - step: 143, loss: 38.54213333129883
2025-01-08 03:50:51,377 - lyh - INFO - time: 37.0071542263031
2025-01-08 03:51:00,156 - lyh - INFO - [MODEL EVAL] Epoch 001 cora {0: [54.43, 50.68], 1: [57.93, 53.94], 2: [59.78, 55.46], 3: [60.7, 56.19], 4: [61.07, 56.31], 5: [61.25, 56.54], 6: [61.81, 56.98], 7: [62.36, 57.42], 8: [62.36, 57.48], 9: [61.25, 55.99]}
2025-01-08 03:52:20,433 - lyh - INFO - [MODEL EVAL] Epoch 001 pubmed {0: [58.16, 57.86], 1: [64.91, 64.42], 2: [66.78, 66.13], 3: [69.57, 68.75], 4: [71.65, 70.67], 5: [73.02, 72.02], 6: [73.66, 72.56], 7: [74.04, 72.79], 8: [74.19, 72.74], 9: [74.06, 72.39]}
2025-01-08 03:53:14,779 - lyh - INFO - [MODEL EVAL] Epoch 001 wikics {0: [57.67, 53.53], 1: [60.74, 55.61], 2: [61.94, 56.41], 3: [62.75, 56.61], 4: [62.37, 56.02], 5: [62.02, 54.78], 6: [60.62, 52.9], 7: [58.05, 50.92], 8: [56.26, 50.24], 9: [52.46, 47.2]}
2025-01-08 03:53:14,779 - lyh - INFO - Epoch: 0, Step: 143, acc: [[61.25, 55.99], [74.06, 72.39], [52.46, 47.2]]
